                                   Fylesystem
=========================================================================================================
                                      DOS
=========================================================================================================
DOS (Disk Operating System) is an operating system that runs from a hard disk drive, usually is refered
to MS-DOS (Microsoft Disk Operating System)

=========================================================================================================
                                      BIOS
=========================================================================================================
Basic Input/Output System => non-volatile firmware used to perform hardware initialization during the
booting process (power on stratup), and to provide runtime services for OS and programs. It`s role is
- initializing and testing hardware components
- loading a boot loader from mass memory
Originally. BIOS firmware was stored in ROM chip on motherboard, but in modern computer systems it is
stored on flash memory so it can be rewritten without removing chip from the motherboard. This allows
easy updates but creates a possibility for the hacking.

Bios knows only about disks on your system and you decide which is active. Than bios firmware will looks
at its start and try to run bootloader, it doesnt know anything else except disks and its start. Real
booting happens above firmware level.

=========================================================================================================
                                      UEFI
=========================================================================================================
UEFI (Unified Extensible Firmware Interface) => successor to the legacy PC BIOS.

---------------------------------------------------------------------------------------------------------
                              UEFI native booting 
---------------------------------------------------------------------------------------------------------
UEFI provides much more infrastructure at the firmware level for handling system boot.  We require the
firmware layer to be capable of reading some specific types of filesystem (FAT12, FAT16 and FAT32 variants
of the FAT format) An 'EFI system partition' is really just any partition formatted with one of the UEFI
spec-defined variants of FAT and given a specific GPT partition type to help formware find it. This way we
don`t rely on 'magic' space at the start of an MBR disk, we can just create,format and mount partitions in
widely understood format and put bootloader code and anything else that they might want the firmware to
read there. Thereis still open question, how we choose which target bootloader execute and how to
configure them

---------------------------------------------------------------------------------------------------------
                               UEFI boot manager
---------------------------------------------------------------------------------------------------------
“The UEFI boot manager is a firmware policy engine that can be configured by modifying architecturally
defined global NVRAM variables. The boot manager will attempt to load UEFI drivers and UEFI applications
(including UEFI OS boot loaders) in an order defined by the global NVRAM variables.”
BIOS firmware has no boot menu, disks connected to your system is your boot menu.
UEFI  provides a confguration mechanism => you can add and remove entries from the "boot menu" using 'efibootmgr'

All entries in `efibootmgr -v` just tells devices to be booted and their order but doesn`t tell how. The
firmware will look through each EFI  system partition on the disk in the order they exist on the disk.
It will look for a file with a specific name and location, like \EFI\BOOT\BOOT{arch}.EFI. From this point,
file contains instructions for booting process.  This mechanism is designed for booting hotpluggable devices.

# TODO note that uefi can be runned in BIOS style
https://cromwell-intl.com/open-source/linux-boot.html
https://www.happyassassin.net/2014/01/25/uefi-boot-how-does-that-actually-work-then/
=========================================================================================================
                                 Chain loading
=========================================================================================================
Chain Loading. Method used by computer programs to replace the currently executing program with a new
program, using a 'common data area' to pass information from the current program to the new program.

=========================================================================================================
                                  Boot sector
=========================================================================================================
Boot sector. Region of a hard/floppy/optical disk or other data storage device that contains machine code
to be loaded into RAM by computer built-in firmware. It is a physical sector or section on hard drive that
includes information about how to start the boot process in order to load an OS

On IBM PC compatible machine, the BIOS selects a boot device, then copies the first sector from the device
(MBV/VBR or any executable code), into physical memory at address 0x7C00

Exmaple of boot sector:
- Master Boot Record (MBR)
- Volume Boot Record (VBR)
=========================================================================================================
                                   Partitions
=========================================================================================================
A partition is a logically independent division on a HDD. There are next types of partitions:
1. Primary partition. Any of four possible first-level partitions into which a hard disk drive on
IBM-compatible personal computer can be divided. This is an artifact of DOS (Disk Opearting system)
2. Logical partition (extended partition). Sub-partitons of primary partition

Active partition => one that contains the OS that computer attempts to load into memory by default when
it is started ir restarted. Only primary partition, and only one that is not used as the logical partition?
can be designated as the active partition.

There are two main types of partition table available. Master Boot Record (MBR) and GUID Partition Table (GPT).

Master Boot Record (MBR) => the first 512 bytes of a storage device, containing OS bootloader and the
storage device`s partition table. Table holds information where partitions start and begin, which are
bootable. Usually this 512 bytes holds instructions to load larger boot loader (maybe from another
partition on a drive?). If it is overwritten, your OS won`t boot. On Linux, MBR will typically hold the
GRUB boot loader. Its limitations =>
- works with disks up tp 2 TB in size but no more
- supporst up to 4 primary partitions
Its inconvinients =>
- you need special utilities to write the MBR and the only way to find out what`s in one to dd the contents
  out and examine them
- thereis not enough place for moderm bootloaders, they will install a small part of themselfves to the
  MBR and rest to the empty space on the disk between the MBR`s end and start of the first partition
  (note that there is no rule that partition can not be started after MBR).
- no layer / mechanism for selecting boot targets other than disks
- boot behaviour cannot be configured above firmware 

It is looking for a device starting with a 512-byte block which ends with the boot signature 0x55AA. The
first 446 bytes of the boot block hold the boot loader, followed by 64 bytes for the partition table, then
those final two bytes of the boot signature. 

GUID partition table (GPT) => standart for doing partition tables - the information at the start of a disk
that defines what partitions that disk contains.

... TODO note that GPT is backward capable to MBR

=========================================================================================================
                           PXE. Preboot Execution Env
=========================================================================================================
PXE. Pronounced as pixie. Specification describes a standardized client–server environment that boots a
software assembly, retrieved from a network, on PXE-enabled clients. On the client side it requires only
a PXE-capable network interface controller (NIC), and uses a small set of industry-standard network
protocols such as DHCP and TFTP. 

=========================================================================================================
                                  Peripherals
=========================================================================================================
Adapter. Hardware device / software component that converts data from one presentation form to another.
Almost every peripheral device uses an adapter to communicate with system bus:
- Display adapter - used to transmit signal to monitor
- USB adapters for printers, keyboards and mice, among others
- Network adapter required to attach to any network

=========================================================================================================
                           USB(Universal Serial Bus)
=========================================================================================================
USB Host => host system, resposible for controling the communication with USB devices
Host computer contains two layers:
- USB host controller hardware layer
- USB host controller software layer (device drivers)
 

USB Host Controller => hardware inside computer that provides an interface for transferring streams of
data between the host computer and the USB devices. It is a card that easily installs into any available
slot in the motherboard or PCI card.?
The back-facing plate of the controller provides two or more hi-speed USB ports
It is DMA capable

USB (Universal Serial Bus) is host controlled, there can only be one host per bus.

HCI (Host Controller Interface). Register-level interface that enables a host controller for USB to
communicate with a host controller driver in software. There are next types of HCI:
- UHCI (Universal Host Controller Interface)
- EHCI (Enhanced Host Controller Intreface)
- XHCI (Extensible Host Controller Intreface)
- VHCI (Virtual Host Controller Intreface)

USB is a host controlled bus where all data transfers are initiated and controlled by the host on a scheduled
basis. All USB peripherals are slaves responding to host commands with the only exception of "remote wakeup"
command which USB device can issue to make the suspended host active.

Formating a drive, means to prepare the chosen partition on the drive to be used by an operating system
by deleting all of the data and setting up a file system

What `bus` in name means? is this interface connection to controller is a bus. 
Are USB controller and USB host the same thing ? probably yes
http://www.usblyzer.com/usb-system-architecture-components.html
=========================================================================================================
                                     Sysfs
=========================================================================================================
Pseudo file system provided by the Linux kernel that exports information about kernel subsystems, hardware
devices, and associated device drivers from the kernel`s device model to user space through virtual files.
Virual files are computed on request or are a mapping into a different file system used as backing store.

A file system consists of two or three layers, which are explicitly separated, and sometimes the functions
are combined. Example of layers =>
- Logica file system => responsible for interaction with the user application. It provides the API
  for file operations OPEN, CLOSE, READ ... and passes the requested operation to the layer below it
  for processing. The logical file system manages open file table entries and per-process file descriptors
  The layer provides file acess, directory operations, security and protection
- Virtual files system => interface that allows support for multiple concurrent instances of physical
  file systems
- Physical file system => this layer is concerned with the physical operation of yhe storage device

Hot plugging (hot swapping) => the ability to add and remove devices to a computer system while the computor
is running and have the OS automatically recognize the change

https://cromwell-intl.com/open-source/sysfs.html
---------------------------------------------------------------------------------------------------------
                           Sysfs device path example
---------------------------------------------------------------------------------------------------------
/sys is an in-memory file system, meaning that kernel data structures appear to the user as a tree of
directories, files, and symbolic links. Since filesystem resides in RAM, there is no physical disk access
and files have no size (there is an exception). 

https://cromwell-intl.com/open-source/sysfs.html
http://www.linux-usb.org/FAQ.html#i6
=========================================================================================================
                                  Device files
=========================================================================================================
On Unix and Unix-like systems, hardware devices are accessed through special files (also called device
files or nodes) located in the /dev directory. These files are read from and written to just like normal
files, but instead of writing and reading data on a disk, they communicate directly with a kernel driver
which then communicates with the hardware.
In ls -l /dev 'b' in rights means that this is block device, another possible value is 'c' for character
device (like sound card). Numbers after group means minor and major device number. Major indicates which
kernel module has detected the devices (is associated with that device) while the minoe number indicates
which specific device it is (kernel returns pointer to device or you can find it by its number in device
table, kernel doesn`t know anything about minor numbers beyond the fact that they refer to devices
implemented by your driver)
    brw-rw----    1 root     root        8,   0 Jan  1 01:00 /dev/sda

Your own device node can be created with
    mknod name <type c/d> <min> <maj>

/dev is automatically filled by utils like 'udev' that just monitors device events and calls 'mknod / rm'
Some of what you find in /dev are pseudo-devices not coresponding to hardware. These provide functions
provided by the kernel, for instance /dev/zero, an endless source of all-zero bytes.

https://cromwell-intl.com/open-source/sysfs.html
=========================================================================================================
                                     Mount
=========================================================================================================
Filesystem. Hierarchy of directories that is located on a single partition (physical?) or other devices
like DVD, CDROM ... and has a single filesystem type (method for organizing data).

UUID. Universally Unique IDentifier. 128 bit identifier used in Linux to identify disk in the /etc/fstab
file. It was created as more flexiable solution to mount names like sda,sdb,... .
If you wanted to add partion as sdb, when one is alredy defined, you would need to change fstab which
involved many issues? It can be used to give persistant namings for devices, just configure it in fstab.

GUID. Globally Unique IDentifier. Microsoft`s implementation of UUID.

Root Partition. Start of filesystem denoted with '/'
Mounting creates a view of a storage device as a directory tree.

Mounting/Unmounting can be seen in kernel logs `dmesg`
All partitions of a system can be found in /etc/mtab
The kernel also keeps information for /proc/mounts, which lists all currently mounted partitions.
For troubleshooting purposes, if there is a conflict between /proc/mounts and /etc/mtab information,
the /proc/mounts data is always more current and reliable than /etc/mtab.

https://backdrift.org/how-to-use-bind-mounts-in-linux
https://unix.stackexchange.com/questions/198590/what-is-a-bind-mount
---------------------------------------------------------------------------------------------------------
                                   Swap space
---------------------------------------------------------------------------------------------------------
Swap space => it is a space used to substitute disk space for RAM when real RAM fills up and more space is
needed. The memory management program swaps infrequently used pages of memory out to a special partition
deisgnated for paging or swapping. The total amount of memory in a Linux is the RAM plus swap space and is
referred to as virtual memory Linux provides two types of swap space. The first, and the most common one
is swap partion. The second is swap file, regular file that is created and preallocated to a spefied size.
This solution can be used if it is not optinal to make additional partiotion and should avoided unless
absolutely necessary. To make file or partition become a swap space, issue `mkswap` on it?.Size of swapspace
depends on RAM size and for devices with 2-8GB of RAM, same size swap is enough.

Trashing => state when virtual memory become nearly full and CPU spends more time on swapping pages then
doing real work

Mounting means ... `TODO`
To mount disk issue `mount [-t fstype] <disk> <mountpoint>`, filesystem type is optional. To unmount call
`umount` with <disk> or <mountpoint> as argument. To let process finish, on unmounter filesystem use
option -l (lazy), which will clean all references to the filesystem as soon as it is not busy anymore.
umount -a -t cifs -l

? Unmouting disk will remove both device and mount ?
? Unmounting mount point will only remove mount point without device?

Disks that are mounted on boot are saved in /etc/fstab, it can be edited for your custom devices. Now you
are left to remount all disks from file with `mount -a`

---------------------------------------------------------------------------------------------------------
                            Consistent device naming
---------------------------------------------------------------------------------------------------------
Devices names are abbreviation of fullname + device order + partition number
For instance => [fd, sd, hd][a. b. c. d][1..]
- hd refers to an IDE-type drive
- sd refers to a SCSI drive in general, for SATA drives and CD/DVD
- fd is floppy disk

---------------------------------------------------------------------------------------------------------
                                     Cli
---------------------------------------------------------------------------------------------------------
To list filesystems issue on of the following
mount -v
cat /proc/mounts
cat /proc/mtd
cat /etc/mtab       # old variant of fstab that may not support some advanced Linux features like containers?
cat /etc/fstab      # mounted on boot

mount
    -B <src_path> <dest>  # bind mount; alternate view of a directory tree; like a hard link

If you want to override options used when mounting, use following, options will be appended, last option
wins when ambiguous
mount <dir|device> -o <new_options> 
By default, only root user can mount filesystems, unless it has `noauto,user` options under fstab.
To remount filesystem issue
mount -o remount,<rw/ro> <dev> <path>     # ignores configuration files
mount -o remount,<rw/ro> <path>           # merges options from configuration file

To make all filesystems readony, can be used to prevent any changes before reboot
echo u > /proc/sysrq-trigger
If the filesystem is backed by a block device, you can make the block device read-only
echo 1 > /sys/block/dm-4/ro
echo 1 > /sys/block/sda/sda2/ro
There may be option to force mount/remount filesystem but mostly likely it won`t help either, please
check man to find correct option, since on one system it can be -f but on another, this may mean `fake`.

/dev/disk/by-{label,uuid,partuuid,partlabel}
---------------------------------------------------------------------------------------------------------
                               Mounting USB drive
---------------------------------------------------------------------------------------------------------
lsblk                          # detect usb harddrive based on its size and filesystem
df -h
sudo blkid
sudo fdisk -l
dmesg | grep -ie 'sd\|usb'
sudo lshw
udisksctl                      # disc scanner from package
mkdir /media/usb               # create mount point
mount /dev/<usb> /media/usb    # mount 

? How to detect drives on system of different type hdd ssd flash

=========================================================================================================
                                    FS types
=========================================================================================================
Windows sypport FAT32, NTFS, exFAT. All Windows fs organize block of data into groups called clusters size
of which varies according to the fs and the size of the partition. It is used to overcomem some of the
limitations in addressing inherent to each fs.


NOTE, different OS may or may ot be able to read other OS FS, for instance, most Linux systems easily read
Windows` NTFS.
=========================================================================================================
                           MIME(Internet Media Types)
=========================================================================================================
There are couple ways to identify type of a file. Two main are file extensions used on Windows and
MIME types used on Linux/Max OS. If there is no file extension, Windows will give you an error or provide
window to choose application to read such files. Linux doesn`t relay on file extensionts, yet, they are
often preset for better readability as well as DE may associate default applications to extensions.
DE has database of associations between file extension to MIME type ( /etc/mime.types ) and MIME to
application (/etc/mailcap).
Linux doesnt needs extension to run executable but it needs it to find application that knows how to
open the data.

Utility like `file` determines file type over 3 ways:
1. Filesystem tests. stat family system call are called on file to determine unix file type, e.g. link,
   directory, char/block device, pipe or a socket. Depending on that, the magic tests are made
2. Magic tests. File type is guessed by a database of patterns called the magic file. If the file type
   can`t be found over magic tests, it is considered to be a text file.
3. Language tests. The file is searched for particular string to find out which language it contains.
   Some scripts can be identified over the hashbang in the first line of the script.
It doesn`t matter how many tests are performed, it`s always just a good guess.

MIME is also used within browser transactions to understand type of the file it is working with (if html
then render if another then download ...).
https://unix.stackexchange.com/questions/207276/how-are-file-types-known-if-not-from-file-suffix/207290#207290
https://unix.stackexchange.com/questions/266999/why-does-linux-use-file-extension-to-decide-the-default-program-for-opening-a-fi
=========================================================================================================
                                      Udev
=========================================================================================================
Udev. Linux subsystem that supplies your computer with device events; Linux dynamic device management;
replacement for the Device File System (DevFS) starting with the Linux 2.6 kernel series;
dynamically creates or removes device node files at boot time in the /dev directory for all types of devices
It allows you to identify devices based on their properties, like vendor ID and device ID, dynamically.
udev runs in userspace (as opposed to devfs which was executed in kernel space).  That makes it a
potentially useful utility, and it's well-enough exposed that a standard user can manually script it to
do things like performing certain tasks when a certain hard drive is plugged in.

Udev is now part of systemd, which can be noticed by inspecting systemd rpm package
  $ rpm -ql systemd | grep udev
    /etc/udev
    /etc/udev/hwdb.bin
    /etc/udev/rules.d
    /etc/udev/udev.conf
    /usr/bin/udevadm

/lib/udev/rules.d/           # The default rules directory
/etc/udev/rules.d/           # The custom rules directory. These rules take precedence.

With the `$ udevadm monitor` command, you can tap into udev in real time and see what it sees when you
plug in different devices. Become root and try it.
  udevadm info -a -n /dev/<dev>     # get device info once noticed on monitor 
  udevadm control --reload          # NOTE: you need to reboot device to sync changes
                                    #        but this might work as well
To add such handler, you will need to edit `/etc/udev/rules.d` files, like `80-local.rules`

https://wiki.debian.org/udev
https://opensource.com/article/18/11/udev
https://www.thegeekdiary.com/beginners-guide-to-udev-in-linux/
=========================================================================================================
                                      UBI
=========================================================================================================
UBI (Usorted Block Images) => volume managment sysem for raw flash devices which manages multiple logical
volumes on a single physical flash device and spreads the I/O load across whole flash chip.

An UBI volume is a set of consecutive logical eraseblocks (LEBs). Each logical eraseblock may be mapped
to any physical eraseblock (PEB). This mapping is managed by UBI

=========================================================================================================
                                     Ramfs
=========================================================================================================


https://www.kernel.org/doc/Documentation/filesystems/ramfs-rootfs-initramfs.txt
=========================================================================================================
                              Initrd and Initramfs
=========================================================================================================
That's what initramfs is for: It is a CPIO archive that gets attached to the kernel image (the kernel
image is the container for the initramfs not other way round) either in the kernel image itself, or by
the bootloader at boot time.

That CPIO archive contains an initial rootfs with the modules required to setup all devices to access the
proper root filesystem and some programs to identify those devices, load the modules, do some other
startup tasks remount the proper root file system to / and start /sbin/init

initrd is similar, with the main difference that it is an filesystem image, that may be and usually is
compressed. The kernel must have support for the filesystem used built in and will mount this image as
the initial /.

Since CPIO is simpler by several orders of magnitudes, initramfs is prefered over initrd, as this saves
both the requirement for any filesystem modules being built in and also makes initramfs creation easier.
Instead of having to create an ext2 image, loopdevice mount and populate it, it boils down to a simple
archive creation, not unlike using tar.

However if you compile your kernel with all required drivers and modules built into the kernel image, and
your root file system device has a fixed name in the system you don't need a initramfs as the kernel can
do things by itself then.

initrd/initramfs is optional and not a requirement. bzImage is the pure kernel image and can be booted
directly by the bootloader. However it might be neccesary to execute some tasks (loading filesystem
modules, drivers for disk access, mounting the root file system from some exchangeable media without
fixed name/path, etc.) that would usually require access to a filesystem and userspace tools.

https://stackoverflow.com/questions/6405083/is-it-possible-to-boot-the-linux-kernel-without-creating-an-initrd-image
---------------------------------------------------------------------------------------------------------
There are complicated boot setups, for instance multiple filesystems were detected, /usr with different
tools and drivers is saved on another partition and cannot be used without mounting (what if it contains
tools needed to access itself), or root filesystem is encrypted and kernel cannot find init executable.
To solve this issue, initrd (initial root disk) was created. Initrd is block device named ramdev that
holds all needed tools and scripts for mounting needed systems before givinig control to init. Kernel
executes configuration script 'linuxrc'(or something like this) on this disk, which prepares system, then
switches to real filesystem and calls init.  There are some disadvantages =>
- ramdev is emulated hard disk that uses memory instead of physical disks that requires driver to
  communicate, meaning that kernel should have compiled-in driver 
- Data is duplicated from some another filesystem
- Block device fills cache with used files

To reduce memory usage, initramfs was created
Initramfs. Base filesystem in RAM, based on tmpfs (light filesystem with changable size) which doesnt use
block device. Its content (scripts, utilities, libraries, configs) is archived into cpio, then archive is
gziped and saved with kernel. Bootloader will forward it to kernel which will create tmpfs file system,
unzip archive to there and start init script

http://linuxdevices.org/introducing-initramfs-a-new-model-for-initial-ram-disks-a/
https://stackoverflow.com/questions/15444917/initrd-ramdisk-initramfs-uclinux
=========================================================================================================
                                      LVM
=========================================================================================================
LVM. Logical Volume Managment. System of managing logical volumes, or filesystems, that is much more
advanced and flexible than the traditional method of partitioning a disk into one or more segments and
formatting that partition with a filesystem. You can do the same with gparted and mannual reconfiguration,
though it LVM is better in some parts.

You can think of LVM as "dynamic partitions", meaning that you can create/resize/delete LVM "partitions"
(they're called "Logical Volumes" in LVM-speak) from the command line while your Linux system is running:
no need to reboot the system to make the kernel aware of the newly-created or resized partitions.

here are 3 concepts that LVM manages:
- Volume Group. Named collection of physical and logical volumes.
  Typical systems only need one Volume Group to contain all of the physical and logical volumes on the
  system, and commonly named after the name of the machine.
- Physical Volume. Correspond to disks; they are block devices that provide the space to store logical volumes.
- Logical Volume. Correspond to partitions: they hold a filesystem.
  Unlike partitions though, logical volumes get names rather than numbers, they can span across multiple
  disks, and do not have to be physically contiguous.

---------------------------------------------------------------------------------------------------------
                                     Features
---------------------------------------------------------------------------------------------------------
One of the biggest advantages LVM has is that most operations can be done on the fly, while the system is
running. Most operations that you can do with gparted require that the partitions you are trying to
manipulate are not in use at the time, so you have to boot from the livecd to perform them.

You also often run into the limits of the msdos partition table format with gparted, including only 4
primary partitions, and all logical partitions must be contained within one contiguous extended partition. 


Many Partitions
---------------------------------------------------------------------------------------------------------
If you like to test various Linux distributions, or just different version of Ubuntu, or both, you can
quickly end up with quite a few partitions. With conventional msdos partitions, this becomes problematic
due to its limitations. With LVM you can create as many Logical Volumes as you wish, and it is usually
quite easy since you usually have plenty of free space left. Usually people allocate the entire drive to
one partition when they first install, but since extending a partition is so easy with LVM, there is no
reason to do this. It is better to allocate only what you think you will need, and leave the rest of the
space free for future use. If you end up running out of the initial allocation, adding more space to that
volume is just one command that completes immediately while the system is running normally.

Snapshots
---------------------------------------------------------------------------------------------------------
This is something you simply can not do without LVM. It allows you to freeze an existing Logical Volume
in time, at any moment, even while the system is running. You can continue to use the original volume
normally, but the snapshot volume appears to be an image of the original, frozen in time at the moment
you created it. You can use this to get a consistent filesystem image to back up, without shutting down
the system. You can also use it to save the state of the system, so that you can later return to that
state if you mess things up. You can even mount the snapshot volume and make changes to it, without
affecting the original.

Usage
---------------------------------------------------------------------------------------------------------

https://wiki.ubuntu.com/Lvm
https://askubuntu.com/questions/3596/what-is-lvm-and-what-is-it-used-for
=========================================================================================================
                   RAID. Redundant Array of Inexpensive Disks
=========================================================================================================
A way of logically putting multiple disks together into a single array. The idea then is that these disks
working together will have the speed and/or reliability of a more expensive disk.
Now, the exact speed and reliability you'll achieve from RAID depends on the type of RAID you're using. 

---------------------------------------------------------------------------------------------------------
                                  Failure Rate
---------------------------------------------------------------------------------------------------------
Spinning disk, mechanical HDDs are typically chosen in situations where needs such as speed and
performance fall second to cost.
- Due to physical limitations and the mechanical nature of many high speed moving parts contained in them
- HDDs also have a relatively high failure rate compared to SSDs

RAID is meant to help alleviate both of these issues, depending on the RAID type you use.
Typically, a mechanical hard drive has a 2.5% chance of failure each year of its operation.
This has been proven by multiple reports and no specific manufacturer or model has a dramatic variation
from that 2.5% rate.

SSDs are typically chosen in situations where speed and performance take a priority to cost considerations.
- As they have no moving parts, their ability to both write and read data on them is significantly faster
  than on a HDD (at least 8-10x faster.)
- Failure rate is roughly .5% during each year of is operation, which significantly reduces the risk
  compared to a spinning HDD.  

Because of the dramatic difference between the technologies of HDDs and SSDs, it’s important to state
that some RAID implementations that are great for HDDs are not for SSDs, and vice versa. 
---------------------------------------------------------------------------------------------------------
                                 Types
---------------------------------------------------------------------------------------------------------
1. RAID 0. Striping. Taking any number of disks and merging them into one large volume.
   + This will greatly increase speeds, as you're reading and writing from multiple disks at a time.
   + An individual file can then use the speed and capacity of all the drives of the array.
   - It is NOT redundant. The loss of any individual disk will cause complete data loss.
   - This RAID type is very much less reliable than having a single disk.

   There are rarely a situation where you should use RAID 0 in a server environment.
   You can use it for cache or other purposes where speed is important and reliability/data loss does not
   matter at all. But it should not be used for anything other than that.

   As an example, with the 2.5% annual failure rate of drives, if you have a 6 disk RAID 0 array, you've
   increased your annual risk of data loss to nearly 13.5%.

2. RAID 1. Mirroring. Almost every use case of RAID 1 is where you have a pair of identical disks
   identically mirror/copy the data equally across the drives in the array.
   The point of RAID 1 is primarily for redundancy.
   If you completely lose a drive, you can still stay up and running off the additional drive.

   In the event that either drive fails, you can then replace the broken drive with little to no downtime.
   + Gives you the additional benefit of increased read performance, as data can be read off any of the
     drives in the array
   - You will have slightly higher write latency.
   - Since the data needs to be written to both drives in the array, you'll only have the available
     capacity of a single drive while needing two drives.

3. RAID 5/6. Striping + Distributed Parity. Requires at least 3 drives (RAID 6 requires at least 4 drives).
   It takes the idea of RAID 0, and strips data across multiple drives to increase performance.
   But, it also adds the aspect of redundancy by distributing parity information across the disks.

   In short, with RAID 5 you can lose one disk, and with RAID 6 you can lose two disks, and still maintain
   your operations and data.

   + RAID 5 and 6 will get you significantly improved read performance
   - Write performance is largely dependent on the RAID controller used. For RAID 5 or 6, you will most
     certainly need a dedicated hardware controller. This is due to the need to calculate the parity data
     and write it across all the disks.

   RAID 5 and RAID 6 are often good options for standard web servers, file servers, and other general
   purpose systems where most of the transactions are reads, and get you a good value for your money.
   This is because you only need to purchase one additional drive for RAID 5 (or two additional drives for
   RAID 6) to add speed and redundancy.

   RAID 5 or RAID 6 is not the best choice for a heavy write environment, such as a database server, as it
   will likely hurt your overall performance. 

   NOTE: RAID 5 / RAID 6, if you lose a drive, you’re going to be seriusly sacrificing performance to keep
   your environment operational. Once you replace the failed drive, data will need to be rebuilt out of the
   parity information. This will take a significant amount of the total performance of the array. These
   rebuild times continue to grow more and more each year, as drives get larger and larger.
4. RAID 10. Mirroring + Striping.
   Requires at least 4 drives and is a combination of RAID 1 (mirroring) and RAID 0 (striping).

   + This will get you both increased speed and redundancy.
   + This is often the recommended RAID level if you're looking for speed, but still need redundancy.
   + Improved read and write performance
   + Fast rebuild time of RAID 1
   - Just like RAID 1, you'll only have the capacity of half the drives

   In a four-drive configuration, two mirrored drives hold half of the striped data and another two mirror
   the other half of the data. This means you can lose any single drive, and then possibly even a 2nd
   drive, without losing any data.

---------------------------------------------------------------------------------------------------------
RAID can be implemented at software or hardware level.
1. Software RAID.
   - leverages some of the system’s computing power to manage the RAID configuration.
   - If you’re looking to maximize performance of a system, such with a RAID 5 or 6 configuration,
     it’s best to use a hardware-based RAID card when you’re using standard HDDs.
2. Hardware RAID.  Requires a dedicated controller installed in the server.
   Hardware based RAID card does all the management of the RAID array(s), providing logical disks to the
   system with no overheard on the part of the system itself.

   Additionally, hardware RAID can provide many different types of RAID configurations simultaneously to
   the system. This includes providing a RAID 1 array for the boot and application drive and a RAID-5 array
   for the large storage array.
---------------------------------------------------------------------------------------------------------
NOTE:
- RAID will not protect you against data corruption, human error, or security issues.
  While it can protect you against a drive failure, there are innumerable reasons for keeping backups.
  So do not take RAID as a replacement for backups. If you don’t have backups in place, you’re not ready
  to consider RAID as an option.
- RAID does not necessarily allow you to dynamically increase the size of the array.
  If you need more disk space, you cannot simply add another drive to the array.
  You are likely going to have to start from scratch, rebuilding/reformatting the array
- RAID isn’t always the best option for virtualization and high-availability failover.

https://www.prepressure.com/library/technology/raid
https://www.steadfast.net/blog/almost-everything-you-need-know-about-raid
=========================================================================================================
                             Data buferizing
=========================================================================================================
Most of data on system is buffered at some point, and you may lose it by kill process maintaining this
buffer. There are couple of ways to forcefully flush resources:
1. Try shell sync command without parameters to flush all caches, or with file path, to flush only it.
2. Attach to process with gdb and `call fsync(1)`, it should return 0, then `detach`
3. Override default fopen() from C library with user defined (link with description below)
.  There are already packaged solutions, like `stdbuf -oL`

The write-back cache on a storage device can come in many different flavors. There is the volatile write-back
cache. Such a cache is lost upon power failure. However, most storage devices can be configured to run in
either a cache-less mode, or in a write-through caching mode. Each of these modes will not return success
for a write request until the request is on stable storage.
It is best to assume a volatile cache, and program defensively
Some file systems provide mount options to control cache flushing behavior, eg -o barrier=0,1


https://lwn.net/Articles/457667/
https://superuser.com/questions/1288890/will-buffer-be-automatically-flushed-to-disk-when-a-process-exits
https://serverfault.com/questions/464323/how-do-you-force-a-process-to-flush-the-data-written-to-an-open-file-descriptor
https://superuser.com/questions/764479/force-output-buffer-flush-in-running-program
=========================================================================================================
                              Filesystem commands
=========================================================================================================
cat /proc/mounts                # list mounts
mount -v
stat -f --format=%T /path       # detect filesystem by path

---------------------------------------------------------------------------------------------------------
fsck                            # file system check
                                # Note, there is usually separate fsck for each fs type
     -A                         # all filesystems from /etc/fstab
     -C                         # display progress bar
     -l                         # lock device
     -N                         # show what would be done without changing
     -P                         # run in parallel
     -R                         # skip root, used with -A
     -t                         # types of fs to be checked

---------------------------------------------------------------------------------------------------------
lsof
     +D <path>                  # list open fds and applications using it
     -p <pid>                   # files used by pid

---------------------------------------------------------------------------------------------------------
df                              # disk filesystem
     -h                         # human readable, sizer in power 1024
     -T                         # print file systems

df <path>                       # determines fylesystem of <path>
---------------------------------------------------------------------------------------------------------
du                              # disk usage
    -h                          # human readable
    -s                          # single directory size
    -a                          # filesystem pass

du ~/go | sort -n -r | less  #sort files by size
ncdu                           # nice cli scanner


=========================================================================================================
                                    Debugfs
=========================================================================================================
Linux RAM based file system used by kernel developers to hold usefull information. There are no rulles
affecting it`s usage, it should be used as developer wish. By default it is mounted to /sys/kernel/debug

https://unix.stackexchange.com/questions/377428/how-do-i-generate-the-sys-kernel-debug-tracing-folder-in-kernel-with-yocto-proj
=========================================================================================================
                                Troubleshooting
=========================================================================================================
lsof. Can be used to detect if someone is using a file, and if they are, who. It reads kernel memory in
its search for open files and helps you list all open files.
  $ lsof /dev/null

=========================================================================================================
                                      Misc
=========================================================================================================
                               Moving vs copying
---------------------------------------------------------------------------------------------------------
If a directory is moved within the same filesystem (the same partition), then all that is needed is to
rename the file path of the directory. No data apart from the directory entry for the directory itself
has to be altered.
rename("hello1.txt", "hello2.txt")         = 0

When you mv a file across file systems, it has the same effect as cp + rm: no speed gain (apart from the
fact that you only run one command, and consistency is guaranteed: you don't have to check if cp succeeded
to perform the rm)

When copying directories, the data for each and every file needs to be duplicated. This involves reading
all the source data and writing it at the destination.a
open("hello1.txt", O_RDONLY)               = 3
open("hello2.txt", O_WRONLY|O_CREAT, 0644) = 4
read(3, "Hello, world!\n", 4096)           = 14
write(4, "Hello, world!\n", 14)            = 14
close(3)                                   = 0
close(4)                                   = 0

https://unix.stackexchange.com/questions/454318/why-is-mv-so-much-faster-than-cp-how-do-i-recover-from-an-incorrect-mv-command
https://serverfault.com/questions/360905/whats-faster-for-copying-files-from-one-drive-to-another
---------------------------------------------------------------------------------------------------------
